{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Incorporating MLP in sphinx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off, we'll train a MLP on the walls street journal __wsj__ corpus. This will give us baseline scores.\n",
    "We choose wsj corpus instead of open corpora because we know the accuracy (3%) we have to achieve ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22nd May:** Literature review. output classes of MLP tied states or phone states.According to this paper by J. hinton https://www.clsp.jhu.edu/~samuel/pdfs/scarf_mlp.pdf, Tied states were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**23rd May:** Writing code to generate log spectra from audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used:<br>**pysoundfile**<br>**python_speech_features**\n",
    "\n",
    "Pysoundfile reads audio files in flac format and returns an array\n",
    "python_speech_features provides a logfbank method that takes in a linear array and returns filter bank energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from python_speech_features import logfbank\n",
    "import soundfile as sf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for making log feats \n",
    "#path to corpus\n",
    "corpus= '/media/t-rex/F/wsj/wsj0';\n",
    "#this file contains paths to all flac files\n",
    "f = open('flac_paths.txt','w')\n",
    "    \n",
    "for dir_name,subdir_name,files in os.walk(corpus):\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.flac'):\n",
    "            path_2_file = dir_name+'/'+file;\n",
    "            #print (path_2_file)\n",
    "            f.write(path_2_file+'\\n')\n",
    "            (s,r) =sf.read(path_2_file)\n",
    "            logfeats = logfbank(s,r, winlen=0.025, winstep=0.01, nfilt=40, nfft=1024, lowfreq=250, highfreq=None, preemph=0.97);\n",
    "            np.savetxt('/media/t-rex/F/wsj/wsj_feats/'+file+'.mls',logfeats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**25th May**: Assigning labels to features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used:<br>**Tensorflow**<br>**keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16786, 16786)\n"
     ]
    }
   ],
   "source": [
    "#data prep\n",
    "from random import randint\n",
    "\n",
    "all_feats=[]\n",
    "all_labels=[]\n",
    "\n",
    "root = '/media/t-rex/F/wsj/';\n",
    "feats_file = root+'wsj_feats';\n",
    "\n",
    "zorp= 0\n",
    "for dir_name,subdir_name,files in os.walk(feats_file):\n",
    "    if zorp ==30:\n",
    "        break\n",
    "    for file in files:\n",
    "        if zorp ==30:\n",
    "            break\n",
    "        if file.endswith('.mls'):\n",
    "            if zorp == 30:\n",
    "                break\n",
    "            zorp +=1\n",
    "            path_2_file = dir_name+'/'+file;\n",
    "            \n",
    "            #print (path_2_file)\n",
    "            feat = np.loadtxt('/media/t-rex/F/wsj/wsj_feats/'+file)\n",
    "            n_frames  = feat.shape[0]\n",
    "\n",
    "            #assigning random labels \n",
    "            labels = [randint(0,2999) for i in range(n_frames)]\n",
    "            all_labels+=list(labels)\n",
    "            all_feats += list(feat)\n",
    "            \n",
    "print(len(all_feats),len(all_labels))\n",
    "#test/train split\n",
    "for_training = int(len(all_feats)*0.8)\n",
    "Xtrain = all_feats[:for_training]\n",
    "ytrain = all_labels[:for_training]\n",
    "Xtest = all_feats[for_training:]\n",
    "ytest = all_labels[for_training:]\n",
    "\n",
    "#save as compressed array\n",
    "np.savez(root+'dataa',Xtrain=Xtrain,ytrain=ytrain,Xtest=Xtest,ytest=ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#or save as pickle\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(Xtrain,open(root+\"Xtrain.p\",\"w\"))\n",
    "# pickle.dump(ytrain,open(root+\"ytrain.p\",\"w\"))\n",
    "# pickle.dump(Xtest,open(root+\"Xtest.p\",\"w\"))\n",
    "# pickle.dump(ytest,open(root+\"ytest.p\",\"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**May 26th**: Writing MLP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('/media/t-rex/F/wsj/dataa.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data['Xtrain']\n",
    "y_train = data['ytrain']\n",
    "x_test = data['Xtest']\n",
    "y_test = data['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3358, 40)\n",
      "(3358,)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#diagnotics\n",
    "a = np.array(y_train[:200])\n",
    "print len(a)\n",
    "print a\n",
    "b = np.zeros((len(a), 3000))\n",
    "b[np.arange(len(a)), a] = 1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, activation='relu', input_dim=40))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(3000, activation='softmax'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train,y_train,epochs=10,batch_size=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test, num_classes = 3000)\n",
    "score = model.evaluate(x_test, y_test, batch_size=2000)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**May 27th**: Setting up kaldi to find tied state alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**May 30th**:Refactoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
